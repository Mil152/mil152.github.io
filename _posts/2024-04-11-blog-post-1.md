---
title: 'Literature Notes - MolFormer'
date: 2024-04-11
permalink: /posts/2024/04/blog-post-1/
tags:
  - Literature Notes
  - Cheminformatics
  - Foundation Model
---

The study highlights the capabilities of the Molecule Transformer (M-Transformer), an unsupervised, pretrained molecular language model that excels in predicting molecular properties from SMILES sequences. This model surpasses traditional graph-based models in various benchmarks, efficiently utilizes computational resources by reducing GPU usage by a factor of 60, and accurately captures interatomic relationships. Further exploration into expanding its applicability beyond small organic molecules is recommended.

MolFormer 
======

```mermaid!
graph TD;
    SupervisedML["Supervised Machine Learning Models"]
    GNNs["Graph Neural Networks (GNNs)"]
    SMILES["SMILES Representation"]
    UnsupervisedML["Unsupervised Machine Learning Models"]
    Transformer["Transformer-based Language Models"]
    LinearAttention["Linear Attention Mechanism"]
    PubChem_ZINC["Large Datasets: PubChem & ZINC"]
    MolFormerXL["MolFormer-XL (Linear Attention + Rotary Positional Embeddings)"]
    PropertyPrediction["Molecular Property Prediction"]
    Fingerprints["Chemical Fingerprints (e.g., ECFP8)"]
    RNN["Recurrent Neural Networks"]
    MPNN["Message Passing Neural Networks"]
    SchNet["SchNet"]
    DimeNet["DimeNet"]
    ContrastiveLearning["Contrastive Learning (2D Graph Topology and 3D Geometry)"]
    GeomGCL["GeomGCL"]
    GEM["Geometry Enhanced Modeling (GEM)"]
    SelfSupervised["Self-Supervised Learning Models"]
    ChemicalLanguageModel["Chemical Language Models"]
    MLM["Masked Language Model (MLM)"]
    Pretraining["Pretraining on SMILES"]
    FineTuning["Fine-Tuning"]
    QuantumProperties["Quantum Chemical Property Prediction"]
    PhysiologicalProperties["Physiological Property Prediction"]

    SupervisedML -->|Influence| MolFormerXL
    GNNs -->|Baseline Comparison| MolFormerXL
    SMILES -->|Data Representation| MolFormerXL
    UnsupervisedML -->|Methodology Influence| Transformer
    Transformer -->|Foundation| MolFormerXL
    LinearAttention -->|Efficiency in Scaling| MolFormerXL
    PubChem_ZINC -->|Training Data| MolFormerXL
    MolFormerXL -->|Application| PropertyPrediction
    Fingerprints -->|Input Feature for| GNNs
    Fingerprints -->|Input Feature for| RNN
    RNN -->|Molecular Representation| MPNN
    MPNN -->|Include Geometric Data| SchNet
    MPNN -->|Include Geometric Data| DimeNet
    MPNN -->|Traditional Methodology| GNNs
    GNNs -->|Spatial Interaction Modeling| SchNet
    GNNs -->|Spatial Interaction Modeling| DimeNet
    SchNet -->|Influence| MolFormerXL
    DimeNet -->|Influence| MolFormerXL
    UnsupervisedML -->|Contextual Pre-training| GeomGCL
    GeomGCL -->|Performance Improvement| PropertyPrediction
    GEM -->|Modeling Atom-Bond-Angle Relations| PropertyPrediction
    ContrastiveLearning -->|Used in| GeomGCL
    SelfSupervised -->|Used by| MolFormerXL
    ChemicalLanguageModel -->|SMILES Based| MolFormerXL
    MLM -->|Training Technique| MolFormerXL
    Pretraining -->|Large-Scale Data| MolFormerXL
    FineTuning -->|Task-Specific| PropertyPrediction
    PropertyPrediction --> QuantumProperties
    PropertyPrediction --> PhysiologicalProperties

    style MolFormerXL fill:#f9f,stroke:#333,stroke-width:4px
    style SchNet fill:#bbf,stroke:#333,stroke-width:2px
    style DimeNet fill:#bbf,stroke:#333,stroke-width:2px
    style GeomGCL fill:#bbf,stroke:#333,stroke-width:2px
    style GEM fill:#bbf,stroke:#333,stroke-width:2px
    style QuantumProperties fill:#cff,stroke:#333,stroke-width:2px
    style PhysiologicalProperties fill:#cff,stroke:#333,stroke-width:2px
```

```mermaid!
graph LR;
    MOLFORMER["MOLFORMER Model"]
    Pretraining["Pre-Training"]
    Optimizer["Fused Lamb Optimizer"]
    LinearAttention["Linear Attention"]
    PositionEmbedding["Position Embeddings"]
    Rotary["Rotary Embeddings"]
    Absolute["Absolute Embeddings"]
    Parallelization["Parallelization"]
    GPUs["GPU Cluster"]
    V100["NVIDIA Tesla V100 GPUs"]
    A100["NVIDIA Ampere A100 GPUs"]
    InfiniBand["InfiniBand Network"]
    Ethernet["Ethernet Network"]
    DistributedDataParallel["Distributed Data Parallel (Pytorch)"]
    RDMA["RDMA for GPU-direct"]
    TrainingEnvironment["Training Environment"]
    Dataset["Training on PubChem + ZINC"]
    Epochs["4 Epochs Training"]
    FineTuning["Fine-Tuning Tasks"]

    MOLFORMER --> Pretraining
    Pretraining --> Optimizer
    Pretraining --> LinearAttention
    Pretraining --> PositionEmbedding
    LinearAttention --> Rotary
    LinearAttention --> Absolute
    PositionEmbedding -->|Selected for use| Rotary
    PositionEmbedding -->|Compared to| Absolute
    MOLFORMER --> Parallelization
    Parallelization --> GPUs
    GPUs --> V100
    GPUs --> A100
    Parallelization --> InfiniBand
    Parallelization --> Ethernet
    Parallelization --> DistributedDataParallel
    DistributedDataParallel --> RDMA
    GPUs --> TrainingEnvironment
    TrainingEnvironment --> Dataset
    Dataset --> Epochs
    MOLFORMER --> FineTuning

    style MOLFORMER fill:#f9f,stroke:#333,stroke-width:4px
    style V100 fill:#bbf,stroke:#333,stroke-width:2px
    style A100 fill:#bbf,stroke:#333,stroke-width:2px
```

### Thoughts on MOLFORMER

1. **Geometric Information**: 
   - Unlike GNNs, MOLFORMER does not directly utilize 3D geometric data, which can limit its performance on properties directly influenced by molecular geometry.
   - It compensates by learning spatial relationships implicitly through advanced attention mechanisms.

2. **Scalability**:
   - MOLFORMER excels in scalability thanks to its linear attention mechanism and efficient GPU utilization, making it suitable for large-scale chemical datasets.
   - This scalability is a significant advantage over GNNs, which often struggle with large or complex graphs.

3. **Benchmark Performance**:
   - MOLFORMER is competitive or superior to GNNs on various benchmarks, demonstrating its capability to generalize across a range of molecular properties without explicit geometric data.

4. **Positional Encoding**:
   - Preferring rotary positional embeddings over absolute types highlights its nuanced approach to encoding molecular structures in transformers.

5. **Future Directions**:
   - **Hybrid Models**: Future enhancements might include hybrid models that integrate GNN's geometric sensitivity with transformer efficiency.
   - **Dataset Diversity**: Expanding dataset diversity will be crucial to maintain performance without increasing computational demands.
