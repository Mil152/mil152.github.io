---
title: 'Literature Notes - MolFormer'
date: 2024-04-11
permalink: /posts/2024/04/blog-post-1/
tags:
  - Literature Notes
  - Cheminformatics
  - Foundation Model
---

The study highlights the capabilities of the Molecule Transformer (M-Transformer), an unsupervised, pretrained molecular language model that excels in predicting molecular properties from SMILES sequences. This model surpasses traditional graph-based models in various benchmarks, efficiently utilizes computational resources by reducing GPU usage by a factor of 60, and accurately captures interatomic relationships. Further exploration into expanding its applicability beyond small organic molecules is recommended.

MolFormer 
======

```mermaid!
graph TD;
    SupervisedML["Supervised Machine Learning Models"]
    GNNs["Graph Neural Networks (GNNs)"]
    SMILES["SMILES Representation"]
    UnsupervisedML["Unsupervised Machine Learning Models"]
    Transformer["Transformer-based Language Models"]
    LinearAttention["Linear Attention Mechanism"]
    PubChem_ZINC["Large Datasets: PubChem & ZINC"]
    MolFormerXL["MolFormer-XL (Linear Attention + Rotary Positional Embeddings)"]
    PropertyPrediction["Molecular Property Prediction"]
    Fingerprints["Chemical Fingerprints (e.g., ECFP8)"]
    RNN["Recurrent Neural Networks"]
    MPNN["Message Passing Neural Networks"]
    SchNet["SchNet"]
    DimeNet["DimeNet"]
    ContrastiveLearning["Contrastive Learning (2D Graph Topology and 3D Geometry)"]
    GeomGCL["GeomGCL"]
    GEM["Geometry Enhanced Modeling (GEM)"]

    SupervisedML -->|Influence| MolFormerXL
    GNNs -->|Baseline Comparison| MolFormerXL
    SMILES -->|Data Representation| MolFormerXL
    UnsupervisedML -->|Methodology Influence| Transformer
    Transformer -->|Foundation| MolFormerXL
    LinearAttention -->|Efficiency in Scaling| MolFormerXL
    PubChem_ZINC -->|Training Data| MolFormerXL
    MolFormerXL -->|Application| PropertyPrediction
    Fingerprints -->|Input Feature for| GNNs
    Fingerprints -->|Input Feature for| RNN
    RNN -->|Molecular Representation| MPNN
    MPNN -->|Include Geometric Data| SchNet
    MPNN -->|Include Geometric Data| DimeNet
    MPNN -->|Traditional Methodology| GNNs
    GNNs -->|Spatial Interaction Modeling| SchNet
    GNNs -->|Spatial Interaction Modeling| DimeNet
    SchNet -->|Influence| MolFormerXL
    DimeNet -->|Influence| MolFormerXL
    UnsupervisedML -->|Contextual Pre-training| GeomGCL
    GeomGCL -->|Performance Improvement| PropertyPrediction
    GEM -->|Modeling Atom-Bond-Angle Relations| PropertyPrediction
    ContrastiveLearning -->|Used in| GeomGCL

    style MolFormerXL fill:#f9f,stroke:#333,stroke-width:4px
    style SchNet fill:#bbf,stroke:#333,stroke-width:2px
    style DimeNet fill:#bbf,stroke:#333,stroke-width:2px
    style GeomGCL fill:#bbf,stroke:#333,stroke-width:2px
    style GEM fill:#bbf,stroke:#333,stroke-width:2px

```

Thoughts:
 - Compare to GNN, geometric info missing in Transformer
 - But it enables scale up