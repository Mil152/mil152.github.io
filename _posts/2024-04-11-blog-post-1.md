---
title: 'Literature Notes - MolFormer'
date: 2024-04-11
permalink: /posts/2024/04/blog-post-1/
tags:
  - Literature Notes
  - Cheminformatics
  - Foundation Model
---

The study highlights the capabilities of the Molecule Transformer (M-Transformer), an unsupervised, pretrained molecular language model that excels in predicting molecular properties from SMILES sequences. This model surpasses traditional graph-based models in various benchmarks, efficiently utilizes computational resources by reducing GPU usage by a factor of 60, and accurately captures interatomic relationships. Further exploration into expanding its applicability beyond small organic molecules is recommended.

MolFormer 
======

![MolFormer](/images/posts/2024-04-11-1.png "MolFormer Chart")

Thoughts:
 - Compare to GNN, geometric info missing in Transformer
 - But it enables scale up

```mermaid!
pie title Pets adopted by volunteers
  "Dogs" : 386
  "Cats" : 85
  "Rats" : 35
```
```mermaid!
 graph TD;
    SupervisedML["Supervised Machine Learning Models"]
    GNNs["Graph Neural Networks (GNNs)"]
    SMILES["SMILES Representation"]
    UnsupervisedML["Unsupervised Machine Learning Models"]
    Transformer["Transformer-based Language Models"]
    LinearAttention["Linear Attention Mechanism"]
    PubChem_ZINC["Large Datasets: PubChem & ZINC"]
    MolFormerXL["MolFormer-XL (Linear Attention + Rotary Positional Embeddings)"]
    PropertyPrediction["Molecular Property Prediction"]
    
    SupervisedML -->|Influence| MolFormerXL
    GNNs -->|Baseline Comparison| MolFormerXL
    SMILES -->|Data Representation| MolFormerXL
    UnsupervisedML -->|Methodology Influence| Transformer
    Transformer -->|Foundation| MolFormerXL
    LinearAttention -->|Efficiency in Scaling| MolFormerXL
    PubChem_ZINC -->|Training Data| MolFormerXL
    MolFormerXL -->|Application| PropertyPrediction

    style MolFormerXL fill:#f9f,stroke:#333,stroke-width:4px
```